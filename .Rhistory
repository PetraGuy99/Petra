install.packages("leaflet")
# Load library
library(leaflet)
polygon_coords <- list(
list(c(51.5074, -0.1278)),  # London
list(c(51.5155, -0.1025)),  # Islington
list(c(51.4954, -0.1446))   # Westminster
)
leaflet() %>%
addTiles() %>%
setView(lng = -0.1278, lat = 51.5074, zoom = 10) %>%
addPolygons(
lng = c(-0.1278, -0.1025, -0.1446),
lat = c(51.5074, 51.5155, 51.4954),
color = "blue"
)
polygon_coords <- matrix(c(
-2.3202680, 51.0936780, # SE
-2.321158,51.09239,  # SW
-2.320709,51.093854, # SW
-2.319996 , 51.094244 # Close the polygon
), ncol = 2, byrow = TRUE)
# Create leaflet map centered in the UK and add polygon
leaflet() %>%
addTiles() %>%
setView(lng = -0.1278, lat = 51.5074, zoom = 10) %>%
addPolygons(
lng = c(-0.1278, -0.1025, -0.1446),
lat = c(51.5074, 51.5155, 51.4954),
color = "blue"
)
polygon_coords <- matrix(c(
-2.3202680, 51.0936780, # SE
-2.321158,51.09239,  # SW
-2.320709,51.093854, # SW
-2.319996 , 51.094244 # Close the polygon
), ncol = 2, byrow = TRUE)
polygon <- st_polygon(list(polygon_coords))
install.packages("ggplot2")
install.packages("sf")
install.packages("rnaturalearth")
install.packages("rnaturalearthdata")
# Load libraries
library(ggplot2)
library(sf)
library(rnaturalearth)
library(rnaturalearthdata)
# Get world map data, including the UK
world <- ne_countries(scale = "medium", returnclass = "sf")
polygon <- st_polygon(list(polygon_coords))
polygon_coords <- matrix(c(
-2.3202680, 51.0936780, # SE
-2.321158,51.09239,  # SW
-2.320709,51.093854, # SW
-2.319996 , 51.094244,
-2.3202680, 51.0936780
# Close the polygon
), ncol = 2, byrow = TRUE)
polygon <- st_polygon(list(polygon_coords))
sf_polygon <- st_sfc(polygon, crs = 4326)
# Plot using ggplot2
ggplot(data = world) +
geom_sf() +
geom_sf(data = sf_polygon, fill = "lightblue", color = "blue") +
coord_sf(xlim = c(-6, 2), ylim = c(50, 60)) + # UK bounding box
theme_minimal() +
labs(title = "Polygon around London")
# Polygon coordinates (longitude, latitude) around London
polygon_coords <- matrix(c(
-2.3202680, 51.0936780, # SE
-2.321158,51.09239,  # SW
-2.320709,51.093854, # SW
-2.319996 , 51.094244,
-2.3202680, 51.0936780
# Close the polygon
), ncol = 2, byrow = TRUE)
# Convert to sf polygon object
polygon <- st_polygon(list(polygon_coords))
sf_polygon <- st_sfc(polygon, crs = 4326)
ggplot(data = world) +
geom_sf() +
geom_sf(data = sf_polygon, fill = "lightblue", color = "blue") +
coord_sf(xlim = c(-6, 2), ylim = c(50, 60)) + # UK bounding box
theme_minimal() +
labs(title = "Polygon around London")
polygon_coords <- list(
list(c(-2.3202680, 51.0936780)),  # London
list(c(-2.321158,51.09239)),  # Islington
list(c( -2.320709,51.093854)),
list(c(-2.319996 , 51.094244)),
list(c( -2.3202680, 51.0936780))
)
# Create leaflet map centered in the UK and add polygon
leaflet() %>%
addTiles() %>%
setView(lng = -0.1278, lat = 51.5074, zoom = 10) %>%
addPolygons(
lng = c(-0.1278, -0.1025, -0.1446),
lat = c(51.5074, 51.5155, 51.4954),
color = "blue"
)
leaflet() %>%
addTiles() %>%
setView(lng = -2.3202680, 51.0936780, zoom = 10) %>%
addPolygons(
lng = c(-2.3202680, -2.321158, --2.321158, -2.319996, -2.3202680),
lat = c(51.0936780, 51.09239, 51.093854, 51.094244,51.0936780 ),
color = "blue"
)
# Create leaflet map centered in the UK and add polygon
leaflet() %>%
addTiles() %>%
setView(lng = -2.3202680, 51.0936780, zoom = 10) %>%
addPolygons(
lng = c(-2.3202680, -2.321158, -2.321158, -2.319996, -2.3202680),
lat = c(51.0936780, 51.09239, 51.093854, 51.094244,51.0936780 ),
color = "blue"
)
# Create leaflet map centered in the UK and add polygon
leaflet() %>%
addTiles(
urlTemplate = "https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png",
attribution = 'Map data © <a href="https://openstreetmap.org/copyright">OpenStreetMap</a> contributors'
) %>%
addProviderTiles("OpenStreetMap.Mapnik") %>%
setView(lng = -2.3202680, 51.0936780, zoom = 10) %>%
addPolygons(
lng = c(-2.3202680, -2.321158, -2.321158, -2.319996, -2.3202680),
lat = c(51.0936780, 51.09239, 51.093854, 51.094244,51.0936780 ),
color = "blue"
)
library(leaflet)
#Nick Hoare - Inoculated treatment
leaflet() %>%
addTiles(
urlTemplate = "https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png",
attribution = 'Map data © <a href="https://openstreetmap.org/copyright">OpenStreetMap</a> contributors'
) %>%
addProviderTiles("OpenStreetMap.Mapnik") %>%
setView(lng = -2.3202680, 51.0936780, zoom = 10) %>%
addPolygons(
lng = c(-2.3202680, -2.321158, -2.321158, -2.319996, -2.3202680),
lat = c(51.0936780, 51.09239, 51.093854, 51.094244,51.0936780 ),
color = "blue"
)
leaflet() %>%
addTiles(
urlTemplate = "https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png",
attribution = 'Map data © <a href="https://openstreetmap.org/copyright">OpenStreetMap</a> contributors'
) %>%
addProviderTiles("OpenStreetMap.Mapnik") %>%
setView(lng = -2.3202680, 51.0936780, zoom = 10) %>%
addPolygons(
lng = c(-2.3202680, -2.321158, -2.321158, -2.319996, -2.3202680),
lat = c(51.0936780, 51.09239, 51.093854, 51.094244,51.0936780 ),
color = "blue"
)
setwd('C:/dev/code/Petra')
library(tidyr)
library(dplyr)
library(purrr)
library(writexl)
library(readxl)
#read in first blast shhet
blast1 = read_excel('../../data/blast.xlsx', sheet = 'blast1')
View(blast1)
View(blast1)
#read in first blast shhet
blast1 = read_excel('../../data/blast.xlsx', sheet = 'blast1', col_names = F)
View(blast1)
#1. remove rows starting 'Length','Score','Identities','Strand','Sbjct'
filter1 =blast1[!grepl("^(Length|Score|Identities|Strand|Sbjct)", blast1$A), ]
colnames(blast1)
#1. remove rows starting 'Length','Score','Identities','Strand','Sbjct'
filter1 =blast1[!grepl("^(Length|Score|Identities|Strand|Sbjct)", blast1$...1), ]
View(filter1)
#1. remove rows starting 'Length','Score','Identities','Strand','Sbjct'
filter1 =blast1[!grepl("^(Length|Score|Identities|Strand|Sbjct|Query:)", blast1$...1), ]
#remove rows which have ||||| at the start
filter2 <- filter1[!grepl("^\\|+$", filter1[[1]]), ]
View(filter2)
#remove rows where all cols are NA - these were various text lines
filter2 <- filter1[rowSums(is.na(filter1)) < ncol(filter1), ]
View(filter2)
View(filter2)
#remove rows where all cols are NA - these were various text lines
filter2 <- filter1[rowSums(is.na(filter1[, 2:13])) < ncol(filter1[, 2:13]), ]
#remove rows where all cols are NA - these were various text lines, except the rows where the
#OTU number is specified
filter2 <- filter1[!(rowSums(is.na(filter1)) == ncol(filter1) & !grepl("^Query \\d+ of \\d+: OTU_", filter1[[1]])), ]
#remove rows where all cols are NA - these were various text lines
filter2 <- filter1[rowSums(is.na(filter1[, 2:13])) < ncol(filter1[, 2:13]) | grepl("^Query \\d+ of \\d+: OTU_", filter1[[1]]), ]
blast1 = read_excel('../../data/blast.xlsx', sheet = 'blast1', col_names = F)
#1. remove rows starting 'Length','Score','Identities','Strand','Sbjct','Query:'
filter1 =blast1[!grepl("^(Length|Score|Identities|Strand|Sbjct|Query:)", blast1$...1), ]
#remove rows where all cols are NA - these were various text lines, unless they are the OTU number rows
filter2 <- filter1[rowSums(is.na(filter1[, 2:13])) < ncol(filter1[, 2:13]) | grepl("^Query \\d+ of \\d+: OTU_", filter1[[1]]), ]
#read in first blast shhet
blast1 = read_excel('../../data/blast.xlsx', sheet = 'blast1', col_names = F)
#1. remove rows starting 'Length','Score','Identities','Strand','Sbjct','Query:'
filter1 =blast1[!grepl("^(Length|Score|Identities|Strand|Sbjct|Query:)", blast1$...1), ]
#remove rows where all cols are NA - these were various text lines, unless they are the OTU number rows
filter2 <- filter1[rowSums(is.na(filter1[, 2:13])) < ncol(filter1[, 2:13]) | grepl("^Query \\d+ of \\d+: OTU_", filter1[[1]]), ]
#read in first blast shhet
blast1 = read_excel('../../data/blast.xlsx', sheet = 'blast1', col_names = F)
#1. remove rows starting 'Length','Score','Identities','Strand','Sbjct','Query:'
filter1 =blast1[!grepl("^(Length|Score|Identities|Strand|Sbjct|Query:)", blast1$...1), ]
#remove rows where all cols are NA - these were various text lines, unless they are the OTU number rows
filter2 <- filter1[rowSums(is.na(filter1[, 2:13])) < ncol(filter1[, 2:13]) | grepl("^Query \\d+ of \\d+: OTU_", filter1[[1]]), ]
View(filter2)
View(filter2)
blast1 = read_excel('../../data/blast.xlsx', sheet = 'blast1', col_names = F)
#1. remove rows starting 'Length','Score','Identities','Strand','Sbjct','Query:'
filter1 =blast1[!grepl("^(Length|Score|Identities|Strand|Sbjct|Query:)", blast1$...1), ]
#remove rows where all cols are NA - these were various text lines, unless they are the OTU number rows
filter2 <- filter1[rowSums(is.na(filter1[, 2:13])) < ncol(filter1[, 2:13]) | grepl("^Query \\d+ of \\d+: OTU_", filter1[[1]]), ]
blast1 = read_excel('../../data/blast.xlsx', sheet = 'blast1', col_names = F)
#1. remove rows starting 'Length','Score','Identities','Strand','Sbjct','Query:'
filter1 =blast1[!grepl("^(Length|Score|Identities|Strand|Sbjct|Query:)", blast1$...1), ]
#remove rows where all cols are NA - these were various text lines, unless they are the OTU number rows
filter2 <- filter1[rowSums(is.na(filter1[, 2:13])) < ncol(filter1[, 2:13]) | grepl("^Query \\d+ of \\d+: OTU_", filter1[[1]]), ]
View(filter2)
View(filter2)
#adda column for the OTU number
filter$OTU = NA
#adda column for the OTU number
filter2$OTU = NA
otu_values <- NA  # Temporary variable to store current OTU_x
for (i in 1:nrow(df)) {
match <- regmatches(df[i, 1], regexpr("^Query \\d+ of \\d+: (OTU_\\d+)", df[i, 1]))
if (length(match) > 0) {
otu_values <- match  # Update OTU_x value when a new query row is found
}
df$new_column[i] <- otu_values  # Assign the current OTU_x value to the row
}
for (i in 1:nrow(filter2)) {
match <- regmatches(filter2[i, 1], regexpr("^Query \\d+ of \\d+: (OTU_\\d+)", df[i, 1]))
if (length(match) > 0) {
otu_values <- match  # Update OTU_x value when a new query row is found
}
filter2$new_column[i] <- otu_values  # Assign the current OTU_x value to the row
}
for (i in 1:nrow(filter2)) {
match <- regmatches(filter2[i, 1], regexpr("^Query \\d+ of \\d+: (OTU_\\d+)", filter2[i, 1]))
if (length(match) > 0) {
otu_values <- match  # Update OTU_x value when a new query row is found
}
filter2$new_column[i] <- otu_values  # Assign the current OTU_x value to the row
}
otu_values <- NA  # Temporary variable to store current OTU_x
for (i in 1:nrow(filter2)) {
match <- regmatches(filter2[i, 1], regexpr("^Query \\d+ of \\d+: (OTU_\\d+)", filter2[i, 1]))
if (length(match) > 0) {
otu_values <- match  # Update OTU_x value when a new query row is found
}
filter2$OTU[i] <- otu_values  # Assign the current OTU_x value to the row
}
#adda column for the OTU number
filter2$OTU = NA
otu_values <- NA  # Temporary variable to store current OTU_x
for (i in 1:nrow(filter2)) {
match <- regmatches(filter2[i, 1], regexpr("^Query \\d+ of \\d+: (OTU_\\d+)", filter2[i, 1]))
if (length(match) > 0) {
otu_values <- match  # Update OTU_x value when a new query row is found
}
filter2$OTU[i] <- otu_values  # Assign the current OTU_x value to the row
}
#remove rows where all cols are NA - these were various text lines, unless they are the OTU number rows
filter2 <- filter1[rowSums(is.na(filter1[, 2:13])) < ncol(filter1[, 2:13]) | grepl("^Query \\d+ of \\d+: OTU_", filter1[[1]]), ]
for (i in 1:nrow(filter2)) {
match <- regmatches(filter2[i, 1], regexpr("^Query \\d+ of \\d+: (OTU_\\d+)", filter2[i, 1]))
if (length(match) > 0) {
otu_values <- match  # Update OTU_x value when a new query row is found
}
filter2$OTU[i] <- otu_values  # Assign the current OTU_x value to the row
}
#adda column for the OTU number
filter2$OTU = NA
for (i in 1:nrow(filter2)) {
match <- regmatches(filter2[i, 1], regexpr("^Query \\d+ of \\d+: (OTU_\\d+)", filter2[i, 1]))
if (length(match) > 0) {
otu_values <- match  # Update OTU_x value when a new query row is found
}
filter2$OTU[i] <- otu_values  # Assign the current OTU_x value to the row
}
#delete the rest of the text so it just says OTU_0003 or whatever
filter2$OTU <- sub("^.*?(OTU_\\d+)$", "\\1", filter2$OTU)
#now reove the rest of the rows with NAs, these are the OTU headers before the set of matches
filter3 <- filter2[rowSums(is.na(filter2[, 2:13])) < ncol(filter2[, 2:13]), ]
View(filter3)
View(filter3)
#remove the rows which start 'reference'
filter4 <- filter3[!grepl("^reference", filter3[[1]]), ]
View(filter4)
View(filter4)
View(filter4)
#remove the rows which start 'reference'
filter4 <- filter3[!grepl("^Reference", filter3[[1]]), ]
#only retain useful cols, SH, namse and % match
filter5 = filter4[,c(1,2,8,14)]
View(filter5)
View(filter5)
#only retain useful cols, SH, namse and % match
filter5 = filter4[,c(1,2,5,8,14)]
#add some nice colnames
colnames(filter5) = c('Reference','SH','Name','Percent','OTU')
#remove rows <97%
filter6 = filter5 %>% filter(Percent>=97)
View(filter6)
#add some nice colnames
colnames(filter5) = c('Reference','SH','Genus','Percent','OTU')
filter5$Genus <- sapply(strsplit(filter5$Genus, " "), function(x) x[1])  # Keep the first word as Genus
filter5$species <- sapply(strsplit(filter5$Genus, " "), function(x) ifelse(length(x) > 1, x[2], NA))
#only retain useful cols, SH, namse and % match
filter5 = filter4[,c(1,2,5,8,14)]
#split name into genus and spp
filter5$Genus <- sapply(strsplit(filter5$Genus, " "), function(x) x[1])  # Keep the first word as Genus
#add some nice colnames
colnames(filter5) = c('Reference','SH','Genus','Percent','OTU')
#split name into genus and spp
filter5$Genus <- sapply(strsplit(filter5$Genus, " "), function(x) x[1])  # Keep the first word as Genus
#only retain useful cols, SH, namse and % match
filter5 = filter4[,c(1,2,5,8,14)]
#add some nice colnames
colnames(filter5) = c('Reference','SH','Genus','Percent','OTU')
filter5$species <- sapply(strsplit(filter5$Genus, " "), function(x) ifelse(length(x) > 1, x[2], NA))  # Assign the second word to Species if it exists
#remove rows where spp col is NA
filter6 <- filter5[!is.na(filter5$species), ]
View(filter6)
View(filter6)
#remove rows <97%
filter7 = filter6 %>% filter(Percent>=97)
View(filter7)
View(filter7)
unique(filter7$OTU)
length(unique(filter7$OTU))
#check the OTUs where there are multiple matches
df_multiple <- filter7[!ave(filter7$Genus, filter7$OTU, FUN = function(x) length(unique(x)) == 1), ]
#check the OTUs where there are multiple matches
df_mutiples <- filter7 %>%
group_by(OTU) %>%
filter(n_distinct(Genus) > 1) %>%
ungroup()
View(df_mutiples)
length(unique(filter7$OTU))
length(unique(df_multiples$OTU))
length(unique(df_mutiples$OTU))
df_filtered <- filter7 %>%
group_by(OTU) %>%
filter(n_distinct(Genus) > 1) %>%  # Keep only OTUs where Genus values do not match
slice(which.max(Percent)) %>%  # Keep the row with the highest Percent value for each OTU
ungroup()
View(df_filtered)
View(df_filtered)
df_filtered <- filter7 %>%
group_by(OTU) %>%
filter(n_distinct(Genus) > 1) %>%  # Keep only OTUs where Genus values do not match
filter(n_distinct(Percent) > 1) %>%  # Keep only OTUs where there is variation in Percent
ungroup()
length(unique(df_filtered$OTU))
#now take the top percent match of the multiple matches
df_top_percent <- df_filtered %>%
group_by(OTU) %>%
slice(which.max(Percent)) %>%  # Select the row with the highest Percent value for each OTU
ungroup()
#then the OTUs which didnt have multiple matches - were all the same species for each OTU
df_same_ss <- filter7 %>%
group_by(OTU) %>%
filter(n_distinct(Genus) == 1) %>%  # Keep OTUs where all Genus values are the same
ungroup()
View(df_same_ss)
View(df_same_ss)
df_same_spp <- filter7 %>%
group_by(OTU) %>%
filter(n_distinct(Genus) == 1) %>%  # Keep OTUs where all Genus values are the same
ungroup()
#and then take the first row
df_first_value <- df_same_spp %>%
group_by(OTU) %>%
slice_head(n = 1) %>%  # Select the first row for each OTU
ungroup()
View(df_first_value)
View(df_first_value)
length(unique(filter7$OTU))
blast1_OTUs = rbind.data.frame(df_first_value, df_top_percent)
length(unique(filter5))
length(unique(filter5$OTU))
View(filter5)
length(unique(filter51$OTU))
length(unique(filter1$OTU))
length(unique(filter1$OTU))
length(unique(filter2$OTU))
length(unique(filter3$OTU))
length(unique(filter4$OTU))
length(unique(filter5$OTU))
length(unique(filter6$OTU))
length(unique(filter7$OTU))
length(unique(df_filtered$OTU))
length(unique(df_same_spp$OTU))
